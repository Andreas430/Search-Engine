{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification\n",
    "\n",
    "The system must provide a function ``search``, with the following specification:\n",
    "```\n",
    "def search(query, ordering = 'normal', count = 10):\n",
    "  ...\n",
    "```\n",
    "\n",
    "It `print`s out the results of the search, subject to the following rules:\n",
    "1. It selects from the set of all recipes that contain __all__ of the words in the query (the positions/order of the words in the recipe are to be ignored).\n",
    "2. It orders them based on the provided ordering (a string, meaning defined below).\n",
    "3. It `print`s the top `count` matches only, preserving the order from best to worst. Must `print` just their title, one per line. Must be less than `count` if the search is specific enough that less than `count` recipes match.\n",
    "\n",
    "As an aside, do not worry about memory usage. If duplicating some data can make your code faster/neater then feel free.\n",
    "\n",
    "\n",
    "\n",
    "### Data set\n",
    "\n",
    "A file, `recipes.json` is provided, containing 17K recipes. It can be parsed into a Python data structure using the [`json`](https://docs.python.org/3/library/json.html) module. It is a list, where each recipe is a dictionary containing various keys:\n",
    "* `title` : Name of recipe; you can assume these are unique\n",
    "* `categories` : A list of tags assigned to the recipe\n",
    "* `ingredients` : What is in it, as a list\n",
    "* `directions` : List of steps to make the recipe\n",
    "* `rating` : A rating, out of 5, of how good it is\n",
    "* `calories` : How many calories it has\n",
    "* `protein` : How much protein is in it\n",
    "* `fat` : How much fat is in it\n",
    "\n",
    "Note that the data set was obtained via web scrapping and hence is noisy - every key except for `title` is missing from at least one recipe. Your code will need to cope with this.\n",
    "\n",
    "You will probably want to explore the data before starting, so you have an idea of what your code has to deal with.\n",
    "\n",
    "Data set came from https://www.kaggle.com/hugodarwood/epirecipes/version/2 though note it has been cleaned it up, by deleting duplicates and removing the really dodgy entries.\n",
    "\n",
    "\n",
    "\n",
    "### Search\n",
    "\n",
    "The search should check the following parts of the recipe (see data set description below):\n",
    "* `title`\n",
    "* `categories`\n",
    "* `ingredients`\n",
    "* `directions`\n",
    "\n",
    "For instance, given the query \"banana cheese\" you would expect \"Banana Layer Cake with Cream Cheese Frosting\" in the results. Note that case is to be ignored (\"banana\" matches \"Banana\") and the words __do not__ have to be next to one another, in the same order as the search query or even in the same part of the recipe (\"cheese\" could appear in the title and \"banana\" in the ingredients). However, all words in the search query __must__ appear somewhere.\n",
    "\n",
    "\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "This is the term for breaking a sentence into each individual word (token). Traditionally done using regular expressions, and Python does have the `re` module, but there is no need to do that here (`re` can be quite fiddly). For matching words your tokenisation must follow the following steps:\n",
    "1. Convert all punctuation and digits into spaces. For punctuation use the set in [`string.punctuation`](https://docs.python.org/3/library/string.html#string.punctuation), for digits [`string.digits`](https://docs.python.org/3/library/string.html#string.digits). You may find [`translate()`](https://docs.python.org/3/library/stdtypes.html#str.translate) interesting!\n",
    "2. [`split()`](https://docs.python.org/3/library/stdtypes.html#str.split) to extract individual tokens.\n",
    "3. Ignore any token that is less than $3$ characters long.\n",
    "4. Make tokens lowercase.\n",
    "\n",
    "When matching words for search (above) or ordering (below) it's only a match if you match an entire token. There are many scenarios where this simple approach will fail, but it's good enough for this exercise. The auto marker will be checking the above is followed! When doing a search the code should ignore terms in the search string that fail the above requirements.\n",
    "\n",
    "\n",
    "\n",
    "### Ordering\n",
    "\n",
    "There are three ordering modes to select from, each indicated by passing a string to the `search` function:\n",
    "* `normal` - Based simply on the number of times the search terms appear in the recipe. A score is calculated and the order is highest to lowest. The score sums the following terms (repeated words are counted multiple times, i.e. \"cheese cheese cheese\" is $3$ matches to \"cheese\"):\n",
    "    * $8 \\times$ Number of times a query word appears in the title\n",
    "    * $4 \\times$ Number of times a query word appears in the categories\n",
    "    * $2 \\times$ Number of times a query word appears in the ingredients\n",
    "    * $1 \\times$ Number of times a query word appears in the directions\n",
    "    * The `rating` of the recipe (if not available assume $0$)\n",
    "\n",
    "* `simple` - Tries to minimise the complexity of the recipe, for someone who is in a rush. Orders to minimise the number of ingredients multiplied by the numbers of steps in the directions.\n",
    "\n",
    "* `healthy` - Order from lowest to highest by this cost function:\n",
    "$$\\frac{|\\texttt{calories} - 510n|}{510} + 2\\frac{|\\texttt{protein} - 18n|}{18} + 4\\frac{|\\texttt{fat} - 150n|}{150}$$\n",
    "Where $n \\in \\mathbb{N}^+$ is selected to minimise the cost ($n$ is a positive integer and $n=0$ is not allowed). This can be understood in terms of the numbers $510$, $18$ and $150$ being a third of the recommended daily intake (three meals per day) for an average person, and $n$ being the number of whole meals the person gets out of cooking/making the recipe. So this tries to select recipes that neatly divide into a set of meals that are the right amount to consume for a healthy, balanced diet. Try not to overthink the optimisation of $n$, as it's really quite simple to do!\n",
    "\n",
    "To clarify the use of the ordering string, to get something healthy that contains cheese you might call `search('cheese', 'healthy')`. In the case of a recipe that is missing a key in its dictionary the rules are different for each search mode:\n",
    "* `normal` - Consider a missing entry in the recipe (e.g. no `ingredients` are provided) to simply mean that entry can't match any search words (because it has none!), but the item is still eligible for inclusion in the results, assuming it can match the search with a different entry.\n",
    "* `simple` - If a recipe is missing either `ingredients` or `directions` it is dropped from such a search result. Because the data is messy if either of these lists is of length $1$ it should be assumed that the list extraction has failed and the recipe is to also be dropped from the search results.\n",
    "* `healthy` - If any of `calories`, `protein` or `fat` is missing the recipe should be dropped from the result.\n",
    "\n",
    "\n",
    "\n",
    "### Extra\n",
    "\n",
    "You may find the [_inverted index_](https://en.wikipedia.org/wiki/Inverted_index) interesting. It's a data structure used by search engines. For each word a user may search for this contains a list of all documents (recipes) that contain the word. This may take a little effort to understand, but the resulting code will be both faster and neater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and reading the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recipes.json') as json_file:\n",
    "    data_recipes = json.load(json_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translators to modify punctuation and digits(make them whitespaces)          \n",
    "translator_pun=str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "translator_dig=str.maketrans(string.digits, ' '*len(string.digits))\n",
    "\n",
    "    \n",
    "def tokenisation_fun(name, i):\n",
    "    \"\"\"This function takes as inputs a key of the dictionary with all recipes,\n",
    "       and the index of the entry and tokenize its value and returns it.\"\"\"\n",
    "    r = data_recipes[i][name]\n",
    "    r = ' '.join(item for item in r)\n",
    "    translator_pun=str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    translator_dig=str.maketrans(string.digits, ' '*len(string.digits))\n",
    "    r = r.translate(translator_pun).translate(translator_dig).lower().split()\n",
    "    r = [item for item in r if len(item) >= 3]\n",
    "    return r\n",
    "\n",
    "def counter_fun(word, text):\n",
    "    \"\"\"This function takes as input a specific word and a string\n",
    "       and returns the number of times the word exists on the string.\"\"\"\n",
    "    \n",
    "    return text.count(word)\n",
    "\n",
    "def normal_fun(word,i):\n",
    "    \"\"\"This function takes as input a word and an index number and calculates a score regarding\n",
    "       to how many times a word exists in the title, category, ingredient and direction of each entry. \n",
    "       Then it returns the score.\"\"\"\n",
    "    \n",
    "    score_t = (8 * counter_fun(word,titles[i]))\n",
    "    score_c = (4 * counter_fun(word,categories[i]))\n",
    "    score_i = (2 * counter_fun(word,ingredients[i]))\n",
    "    score_d = (1 * counter_fun(word,directions[i]))     \n",
    "    score = score_t + score_c + score_i + score_d  \n",
    "    return score\n",
    "      \n",
    "\n",
    "def simple_fun(directions, ingredients):\n",
    "    \"\"\"This function takes as inputs the directions and ingredients of the current entry\n",
    "       and calculates a score according to the length of the list with directions and\n",
    "       ingredients and returns it. The score is the multiplication of the lengths of the lists.\"\"\"\n",
    "    \n",
    "    x = len(directions)\n",
    "    y = len(ingredients)\n",
    "    if x > 1 and y > 1:\n",
    "        return x * y\n",
    "    \n",
    "    \n",
    "def healthy_fun(calories,protein,fat):\n",
    "    \"\"\"This function takes as inputs the calories, protein and fat of the current entry.\n",
    "       The positive integer 𝑛 is selected to minimise the cost_function below. It returns\n",
    "       the score which minimizes the cost_funtion.\"\"\"\n",
    "    best_cost =  np.inf\n",
    "    for n in range(1,5):\n",
    "        cost_function = (abs(calories -510*n)/510) + 2 * (abs(protein -18 *n)/18) + 4 * (abs(fat-150*n)/150) \n",
    "        \n",
    "        if cost_function < best_cost:\n",
    "            best_cost = cost_function\n",
    "    return best_cost\n",
    "       \n",
    "    \n",
    "def common_indices_fun(query):\n",
    "    \"\"\"This function takes as input the query and returns a list with all common indices\n",
    "       where the words in the query belong.\"\"\"\n",
    "    new_list = []\n",
    "    for word in query:\n",
    "        if word in words_dict.keys():\n",
    "            new_list.append(words_dict[word])\n",
    "        else:\n",
    "            return None\n",
    "          \n",
    "        a = list(reduce(set.intersection, [set(item) for item in new_list ]))\n",
    "    return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list = [] # a list of lists, in which each list contains the tokenised words for each entry \n",
    "all_words_list = [] # a list with all the tokenised words from the data\n",
    "titles = [] # a list of lists, in which each list contains the tokenised title's words for each entry\n",
    "categories = [] # a list of lists, in which each list contains the tokenised category's words for each entry\n",
    "ingredients = [] # a list of lists, in which each list contains the tokenised ingredient's words for each entry\n",
    "directions = [] # a list of lists, in which each list contains the tokenised direction's words for each entry\n",
    "\n",
    "# Looping through recipes in our data_recipes and calling the tokenisation_fun to tokenize the words on each entry\n",
    "for i, recipe in enumerate(data_recipes):\n",
    "    \n",
    "    lst = []\n",
    "    title = recipe['title'].translate(translator_pun).translate(translator_dig).lower().split()\n",
    "    titles_list = [ t for t in title if len(t) >= 3]\n",
    "    \n",
    "    # Initially we check whether the specific key exists on each entry and then we tokenize it, using always the entry's index\n",
    "    if 'categories' in recipe.keys(): \n",
    "        categories_list = tokenisation_fun('categories',i)\n",
    " \n",
    "    if 'ingredients' in recipe.keys(): \n",
    "        ingredients_list = tokenisation_fun('ingredients',i)\n",
    "\n",
    "    if 'directions' in recipe.keys():\n",
    "        directions_list = tokenisation_fun('directions',i)\n",
    "\n",
    "\n",
    "    lst = titles_list + categories_list + ingredients_list + directions_list\n",
    "    check_list.append(lst)\n",
    "    all_words_list += lst\n",
    "    titles.append(titles_list)\n",
    "    categories.append(categories_list)\n",
    "    ingredients.append(ingredients_list)\n",
    "    directions.append(directions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a set with all tokenised words. A set contains only unique values, so we just have the unique words that\n",
    "# the data is using.\n",
    "unique_items = set(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary words_dict is a dictionary with key-value pairs where the key is the word and its value is a list\n",
    "# of the indices of entries on which the specific word occurs.\n",
    "\n",
    "words_dict = {}\n",
    "\n",
    "for word in unique_items:\n",
    "    lst_words = []\n",
    "    for i in range(len(check_list)):\n",
    "        if word in check_list[i]:\n",
    "            lst_words.append(i)\n",
    "    words_dict.update({word : lst_words})                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, ordering = 'normal', count = 10):\n",
    "    \"\"\"This function takes as input a query which is a string of words, a definition of the ordering(whether\n",
    "       is 'normal','simple','healthy') and a count number and returns as many recipes as the count number\n",
    "       according to some score being calculated. More specifically, it selects from the set of all recipes\n",
    "       that contain all of the words in the query, it orders them based on the provided ordering, and then \n",
    "       it prints the top count matches only, preserving the order from best to worst\"\"\"\n",
    "    \n",
    "    translator_pun=str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    translator_dig=str.maketrans(string.digits, ' '*len(string.digits))\n",
    "    \n",
    "    # Using the translators above we tokenize the query as well and then we get a list of all tokenised words in the query\n",
    "    query = query.translate(translator_pun).translate(translator_dig).lower().split()\n",
    "    query = [word for word in query if len(word) >= 3]\n",
    "    \n",
    "\n",
    "    indices = common_indices_fun(query)\n",
    "    \n",
    "    if indices == None:\n",
    "        return \n",
    "    else:\n",
    "        \n",
    "        # Creating three dictionaries that contain as their keys the indices of the matched recipes and\n",
    "        # as their values the calculated scores in each ordering definition.\n",
    "        score_normal_dict = {}\n",
    "        score_simple_dict = {}\n",
    "        score_healthy_dict = {}\n",
    "\n",
    "        if ordering == 'normal':\n",
    "            \n",
    "            for i in indices:\n",
    "                normal_score = 0\n",
    "                for word in query:\n",
    "                    normal_score += normal_fun(word,i)\n",
    "\n",
    "                if 'rating' in data_recipes[i]:\n",
    "                    normal_score += data_recipes[i]['rating']\n",
    "                else:\n",
    "                    normal_score += 0\n",
    "\n",
    "                score_normal_dict[i]=normal_score\n",
    "\n",
    "            # Sorting the dictionary according to the score/values and using reverse we get the scores from highest\n",
    "            # to lowest \n",
    "            sorted_normal_score = sorted(score_normal_dict.items(), key=operator.itemgetter(1), reverse = True)[:count]  \n",
    "            \n",
    "            # Finding the corresponding title for each score\n",
    "            for i in sorted_normal_score:\n",
    "                print(data_recipes[i[0]]['title'])\n",
    "\n",
    "        # Doing exactly the same thing as above for the other ordering definitions but now we are not using reverse as\n",
    "        # we want the order to be from lowest to highest. Then printing out the corresponding titles.\n",
    "        elif ordering == 'simple':\n",
    "\n",
    "            for i in indices:\n",
    "                simple_score = simple_fun(data_recipes[i]['directions'],data_recipes[i]['ingredients'])\n",
    "                if simple_score != None:\n",
    "                    score_simple_dict[i]=simple_score\n",
    "\n",
    "            sorted_simple_score = sorted(score_simple_dict.items(), key=operator.itemgetter(1))[:count]\n",
    "            for i in sorted_simple_score:\n",
    "                print(data_recipes[i[0]]['title'])\n",
    "\n",
    "        elif ordering == 'healthy':\n",
    "\n",
    "            for i in indices:\n",
    "                if 'calories' in data_recipes[i].keys() and 'fat' in data_recipes[i].keys() and 'protein' in data_recipes[i].keys():\n",
    "                    healthy_score = healthy_fun(data_recipes[i]['calories'],data_recipes[i]['protein'],data_recipes[i]['fat'])\n",
    "                    score_healthy_dict[i]=healthy_score\n",
    "\n",
    "            sorted_healthy_score = sorted(score_healthy_dict.items(), key=operator.itemgetter(1))[:count]\n",
    "            for i in sorted_healthy_score:\n",
    "                print(data_recipes[i[0]]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana Layer Cake with Cream Cheese Frosting \n",
      "Banana Layer Cake with White Chocolate-Cream Cheese Frosting and Walnuts \n",
      "Banana-Pineapple Layer Cake with Cream Cheese Frosting \n",
      "Fresh Banana Layer Cake \n",
      "Peanut Butter Banana Cream Pie \n",
      "Banana Layer Cake \n",
      "Banana Coconut Crunch Cake \n",
      "Carrot-Banana Cake \n",
      "Persimmon Cake with Cream Cheese Icing \n",
      "Peanut Butter Cheesecake with Caramelized Banana Topping \n",
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%time search('banana cheese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
